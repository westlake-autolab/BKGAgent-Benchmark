# Intro
We propose a biomedical knowledge-graph agent BKGAgent, a multi-agent framework based on LangGraph, which is capable of retrieving information in knowledge graph and cross-validate its correctness with multiple information sources.  
Our framework is comprised of three agents, namely the team leader for the progress control, the KG agent for the information retrieval in KG, and the validation agent for checking the correctness of the information from KG, simulating the workflow of a human research team where a leader supervises the assistants' work and makes the final decision given the feedback from them. Besides, the tool executor is solely responsible for executing the tool agent specified.  
When a user assigns a task, the leader initially breaks down the task and announces the plan. Then the KG agent is activated to retrieve task-related information from the KG. This involves specifying the tool and its arguments to the tool executor, interpreting the tool result, and communicating it back to the leader. After that, the validation agent is called for verification with a similar workflow to that of the KG agent. Finally, a conclusion will be drawn by the leader and returned to the user.  
![kgcheck framework](/assets/img/kgcheck-framwork.png "kgcheck framework")
# Code Framework
```
KGCheck  
|-- README.md  
|-- evaluation
|   `-- evaluate.py
|-- retrieve_toolbox
|   |-- __init__.py
|   |-- corpus_based_retrieve.py
|   `-- web_api.py
|-- base.py                               # Define multi Agents(Leader, KG, Validation, Tool Executor)
|-- prompts.py                              # Define prompts for the agents
|-- agents.py                                 # Main script
```
# Performance
Most agent frameworks fail in KGCheck, which further highlights that KGCheck is a novel and challenging task. It requires agents to first query the knowledge graph, followed by verification through database searches or RAG of literature. Consequently, agents lacking capabilities for KG querying or information retrieval verification cannot complete this task. Therefore, for agents capable of querying knowledge graphs, we selected *KG-Agent* from as representative; for general agent frameworks, we chose the most prominent ones, *AutoGen* and *AutoGPT2*, along with their three improved versions, as well as our *BKGAgent*, for comparison, as shown in Table 8. Both the final results and process are considered for a more robust evaluation. Since the ground truth is either “support” or “refute”, we use Exact Match (EM) as the metric for the final result. For the process, we employ *Qwen2-72B4* to score based on five criteria: understanding the question, reasoning, efficiency, KG processing, and information retrieval. To align the judgments made by LLMs closely with those of humans, we collect 10 agent histories along with human-annotated scores (on a 5-point scale) and prompt the LLM to produce scores that closely resemble human ratings. We take EM as the main metric, while process scores serve as supplementary metrics.
Table 8 shows that BKGAgent outperforms the other agents. KG-Agent achieved an accuracy of only **56.6%**, roughly equivalent to random guessing. This aligns with our expectations, as while it can accurately query information from the knowledge graph, it lacks access to reliable external knowledge sources for verification, leading to hallucinations in the large model’s guesses. Notably, the final accuracy of Vanilla *AutoGen* and *AutoGPT* is quite low, at just over 30%. This underscores the importance of integrating general capabilities with specialized tools to enhance agent performance. Their performance suffers because they are general frameworks that rely on some general capabilities like programming and web searches, which are not robust enough, often resulting in execution failures due to poor code quality. Consequently, they cannot provide answers within the limited interaction turns. Thus, we improved *KG-Agent*, *AutoGen*, and *AutoGPT* by equipping them with tools including KG querying and RAG. We also designed prompts to teach them how to utilize these tools. As a result, KG-Agent w/ our tools, AutoGen w/ our tools, and AutoGPT w/ our tools demonstrate significant improvements, highlighting that the integration of general capabilities with specialized tools enhances the robustness of agent performance.
![kgcheck performance](/assets/img/kgcheck_performance.png "kgcheck performance")